{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For both Smith-Waterman and Needleman-Wunsch algorithms:\n",
    "    a) What are the parameters and variables required for algorithm initialization, execution, and termination?\n",
    "    b) What quantities are returned?\n",
    "    c) What is the runtime complexity?\n",
    "    \n",
    "    Smith-Waterman:\n",
    "        a) Parameters:\n",
    "            Initialization:\n",
    "                a) seq1: First protein sequence\n",
    "                b) seq2: Second protein sequence\n",
    "                c) smat: scoring matrix\n",
    "            Execution:\n",
    "                a) scores: matrix of size M x N that records scores of optimal alignment\n",
    "                b) traceback: matrix of size M x N that records previous cell from which each cell was computed from\n",
    "            Termination:\n",
    "                a) result: optimal alignment string\n",
    "        b) Returns: Optimal sub-alignment with positive scores\n",
    "        c) Complexity: O(MxN)\n",
    "\n",
    "    Needleman-Wunsch:\n",
    "        a) Parameters:\n",
    "            Initialization:\n",
    "                a) seq1: First protein sequence\n",
    "                b) seq2: Second protein sequence\n",
    "                c) smat: scoring matrix\n",
    "            Execution:\n",
    "                a) scores: matrix of size M x N that records scores of optimal alignment\n",
    "                b) traceback: matrix of size M x N that records previous cell from which each cell was computed from\n",
    "            Termination:\n",
    "                a) result: optimal alignment string\n",
    "        b) Returns: Full alignment string of sequence \n",
    "        c) Complexity: O(MxN)\n",
    "\n",
    "2) What functionalities in initialization, execution and termination are shared between these algorithms? Which are not shared?\n",
    "\n",
    "    Initialization: Both methods initialize and M x N matrix where M is the length of seq1 and N is the length of seq2. However, in initializing the SW matrix, you set the first row and first column to 0. \n",
    "    \n",
    "    Execution: Both methods share methods for calculating the score at a particular cell in the matrix. This entails taking the max of the three following scores:\n",
    "        a) Gap from left to right\n",
    "        b) Gap from top to bottom\n",
    "        c) Match\n",
    "    However, SW adds an extra option where you can reset to 0 if the score goes negative.\n",
    "    \n",
    "    Termination: Both methods have to use the traceback matrix to return an alignment. NW must start at the lower rightmost cell and trace back all the way to the start and return that string. SW needs to start at the max element in the matrix and trace back until a 0 is reached, then return the resulting string.\n",
    "\n",
    "3) How does affine-gap based alignment differ from linear-gap alignment in terms of implementation?\n",
    "    \n",
    "    Affine gap alignment requires you to keep track of the gap status of the previous cell when computing the score of the current cell (gap start, match/mismatch, gap extension). To return the optimal alignment you need two extra score matrices, one for seq1 and one for seq2, which keep track of the max score of either opening or extending a gap at the current cell. For the final traceback you would need to interface between the original score matrix and these two extra matrices to get the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./sequences/prot-0088.fa\",'r').read()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SmithWaterman(\"/Users/mtsui1/Documents/Classes/Algs/Project1/scoring_matrices/BLOSUM50.mat\")\n",
    "scores, alignment = sw.align(\"./sequences/test1.fa\", \"./sequences/test2.fa\")\n",
    "print(scores)\n",
    "print(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Pospairs and negpairs into a list\n",
    "\n",
    "f = open(\"./scoring_matrices/Pospairs.txt\",'r')\n",
    "lines = f.readlines() # skip header\n",
    "pos_pairs = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = [i for i in line.split(\" \") if i != \"\"]\n",
    "    pos_pairs.append(line)\n",
    "    \n",
    "f = open(\"./scoring_matrices/Negpairs.txt\",'r')\n",
    "lines = f.readlines() # skip header\n",
    "neg_pairs = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = [i for i in line.split(\" \") if i != \"\"]\n",
    "    neg_pairs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align.algs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SmithWaterman(\"/Users/mtsui1/Documents/Classes/Algs/Project1/scoring_matrices/BLOSUM50.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.set_gap_open(-11)\n",
    "sw.set_gap_extend(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = {}\n",
    "for pair in pos_pairs:\n",
    "    print(pair)\n",
    "    score = sw.align(pair[0], pair[1], return_alignment=False)\n",
    "\n",
    "    pos_scores[(pair[0], pair[1])] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = algs.NeedlemanWunsch(\"/Users/mtsui1/Documents/Classes/Algs/Project1/scoring_matrices/BLOSUM50.mat\")\n",
    "#sw.set_gap_open(-11)\n",
    "#sw.set_gap_extend(-3)\n",
    "score, alignment = sw.align(\"sequences/test1.fa\", \"sequences/test2.fa\")\n",
    "print(score)\n",
    "print(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_scores = {}\n",
    "for pair in neg_pairs:\n",
    "    print(pair)\n",
    "    score = sw.align(pair[0], pair[1], return_alignment=False)\n",
    "\n",
    "    neg_scores[(pair[0], pair[1])] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_scores = list(pos_scores.values())\n",
    "\n",
    "n_scores = list(neg_scores.values())\n",
    "\n",
    "all_scores = p_scores + n_scores\n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(x=all_scores, bins='auto', rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of aligment scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is centered around scores from ~25-50, meaning that most alignments are falling into this score range. There are some higher scoring outliers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_score = np.mean(all_scores)\n",
    "print(\"Average score (threshold): \" + str(avg_score))\n",
    "\n",
    "# predict that the scores higher than threshold are True\n",
    "predicted = all_scores > avg_score\n",
    "\n",
    "# First 50 scores are true positive, last 50 are true negative\n",
    "actual = list(np.repeat(True, 50))  + list(np.repeat(False, 50))\n",
    "\n",
    "df = pd.DataFrame({'predicted': predicted, 'actual': actual})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(df['actual'], df['predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sensitivity is TP/TP+FN\n",
    "sensitivity = 22/(22+28)\n",
    "print(\"sensitivity: \" + str(sensitivity))\n",
    "\n",
    "# specificity = TN/TN+FP\n",
    "specificity = 44/(44+6)\n",
    "print(\"specificity: \" + str(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the average score to threshold seems to give a low sensitivity but high specificity. I notice that there are lots of false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1152c67e2374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mns_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mns_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_tpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(df['actual'], df['predicted'])\n",
    "ns_probs = [False for _ in range(len(df['predicted']))]\n",
    "ns_fpr, ns_tpr, _ = roc_curve(df['actual'], ns_probs)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Local alignment')\n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "ax.set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(df['actual'], df['predicted'])\n",
    "print(\"AUC: \" + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the AUC, using the average score as a threshold, you can see that algorithm performs at least better than the random line (AUC 0.5). However, you cannot determine the performance using just one threshold. You need to try many different thresholds to find the one that maximizes the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SmithWaterman(\"/Users/mtsui1/Documents/Classes/Algs/Project1/scoring_matrices/BLOSUM62.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "3 5\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n",
      "4 5\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 5\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "9 1\n",
      "9 2\n",
      "9 3\n",
      "9 4\n",
      "9 5\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "10 5\n",
      "11 1\n",
      "11 2\n",
      "11 3\n",
      "11 4\n",
      "11 5\n",
      "12 1\n",
      "12 2\n",
      "12 3\n",
      "12 4\n",
      "12 5\n",
      "13 1\n",
      "13 2\n",
      "13 3\n",
      "13 4\n",
      "13 5\n",
      "14 1\n",
      "14 2\n",
      "14 3\n",
      "14 4\n",
      "14 5\n",
      "15 1\n",
      "15 2\n",
      "15 3\n",
      "15 4\n",
      "15 5\n",
      "16 1\n",
      "16 2\n",
      "16 3\n",
      "16 4\n",
      "16 5\n",
      "17 1\n",
      "17 2\n",
      "17 3\n",
      "17 4\n",
      "17 5\n",
      "18 1\n",
      "18 2\n",
      "18 3\n",
      "18 4\n",
      "18 5\n",
      "19 1\n",
      "19 2\n",
      "19 3\n",
      "19 4\n",
      "19 5\n",
      "20 1\n",
      "20 2\n",
      "20 3\n",
      "20 4\n",
      "20 5\n"
     ]
    }
   ],
   "source": [
    "pos_scores_gaps = {}\n",
    "\n",
    "for gap_open in range(1,21):\n",
    "    sw.set_gap_open(-gap_open)\n",
    "    for gap_extend in range(1,6):\n",
    "        scores = []\n",
    "        sw.set_gap_extend(-gap_extend)\n",
    "        for pair in pos_pairs:\n",
    "            score = sw.align(pair[0], pair[1], return_alignment=False)\n",
    "            scores.append(score)\n",
    "        pos_scores_gaps[(gap_open, gap_extend)] = scores\n",
    "        print(gap_open, gap_extend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = {}\n",
    "for k,v in pos_scores_gaps.items():\n",
    "    new[str(k)] = v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pos_scores_gaps.json', 'w') as f:\n",
    "    json.dump(new, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pos_scores_gaps.json') as f:\n",
    "    pos_scores_gaps = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "3 5\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n",
      "4 5\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 5\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "9 1\n",
      "9 2\n",
      "9 3\n",
      "9 4\n",
      "9 5\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "10 5\n",
      "11 1\n",
      "11 2\n",
      "11 3\n",
      "11 4\n",
      "11 5\n",
      "12 1\n",
      "12 2\n",
      "12 3\n",
      "12 4\n",
      "12 5\n",
      "13 1\n",
      "13 2\n",
      "13 3\n",
      "13 4\n",
      "13 5\n",
      "14 1\n",
      "14 2\n",
      "14 3\n",
      "14 4\n",
      "14 5\n",
      "15 1\n",
      "15 2\n",
      "15 3\n",
      "15 4\n",
      "15 5\n",
      "16 1\n",
      "16 2\n",
      "16 3\n",
      "16 4\n",
      "16 5\n",
      "17 1\n",
      "17 2\n",
      "17 3\n",
      "17 4\n",
      "17 5\n",
      "18 1\n",
      "18 2\n",
      "18 3\n",
      "18 4\n",
      "18 5\n",
      "19 1\n",
      "19 2\n",
      "19 3\n",
      "19 4\n",
      "19 5\n",
      "20 1\n",
      "20 2\n",
      "20 3\n",
      "20 4\n",
      "20 5\n"
     ]
    }
   ],
   "source": [
    "neg_scores_gaps = {}\n",
    "\n",
    "for gap_open in range(1,21):\n",
    "    sw.set_gap_open(-gap_open)\n",
    "    for gap_extend in range(1,6):\n",
    "        scores = []\n",
    "        sw.set_gap_extend(-gap_extend)\n",
    "        for pair in neg_pairs:\n",
    "            score = sw.align(pair[0], pair[1], return_alignment=False)\n",
    "            scores.append(score)\n",
    "        neg_scores_gaps[str((gap_open, gap_extend))] = scores\n",
    "        print(gap_open, gap_extend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_scores_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./neg_scores_gaps.json', 'w') as f:\n",
    "    json.dump(neg_scores_gaps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aucs = {}\n",
    "\n",
    "# First 50 scores are true positive, last 50 are true negative\n",
    "actual = list(np.repeat(True, 50))  + list(np.repeat(False, 50))\n",
    "\n",
    "for k,v in pos_scores_gaps.items():\n",
    "    all_scores = pos_scores_gaps[k] + neg_scores_gaps[k]\n",
    "    avg_score = np.mean(all_scores)\n",
    "    pred_scores = all_scores > avg_score\n",
    "    auc = roc_auc_score(actual, pred_scores)\n",
    "    aucs[k] = auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) 0.7400000000000001\n"
     ]
    }
   ],
   "source": [
    "maximum = max(aucs, key=aucs.get)  # Just use 'min' instead of 'max' for minimum.\n",
    "print(maximum, aucs[maximum])\n",
    "# D 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = {}\n",
    "for pair in pos_pairs:\n",
    "    print(pair)\n",
    "    score = sw.align(pair[0], pair[1], return_alignment=False)\n",
    "\n",
    "    pos_scores[(pair[0], pair[1])] = score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
